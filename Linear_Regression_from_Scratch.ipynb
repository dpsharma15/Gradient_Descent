{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression Parameter Estimation example with Gradient Descent\n",
        "\n",
        "Recall our discussion on cost function for linear regression, it looked like this:\n",
        "\n",
        "$$\n",
        "SSE = \\sum e_i^2 = \\sum (y_i - \\hat{y}_i)^2 = \\sum (y_i - \\beta_0 - \\beta_1 x_i)^2\n",
        "$$\n",
        "\n",
        "If we consider entire $X,Y$ data and parameters $\\beta$s to be written in matrix format like this:\n",
        "\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_n\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & x_{11} & x_{21} & \\cdots & x_{p1} \\\\\n",
        "1 & x_{12} & x_{22} & \\cdots & x_{p2} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{1n} & x_{2n} & \\cdots & x_{pn}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\beta = \\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\vdots \\\\\n",
        "\\beta_p\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "JjEororx0hIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression Parameter Estimation with Gradient Descent\n",
        "\n",
        "#### Cost Function (SSE)\n",
        "The sum of squared errors is defined as:\n",
        "\n",
        "$$\n",
        "SSE = \\sum e_i^2 = \\sum (y_i - \\hat{y}_i)^2 = \\sum (y_i - \\beta_0 - \\beta_1 x_i)^2\n",
        "$$\n",
        "\n",
        "In matrix notation:\n",
        "$$\n",
        "SSE = (y - X\\beta)^T(y - X\\beta)\n",
        "$$\n",
        "\n",
        "#### Matrix Definitions\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_n\n",
        "\\end{bmatrix}, \\quad\n",
        "X = \\begin{bmatrix}\n",
        "1 & x_{11} & \\cdots & x_{p1} \\\\\n",
        "1 & x_{12} & \\cdots & x_{p2} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{1n} & \\cdots & x_{pn}\n",
        "\\end{bmatrix}, \\quad\n",
        "\\beta = \\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\vdots \\\\\n",
        "\\beta_p\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "1. Prediction equation:\n",
        "   $$\n",
        "   X\\beta \\rightarrow \\text{predicted values}\n",
        "   $$\n",
        "\n",
        "2. Residual calculation:\n",
        "   $$\n",
        "   Y - X\\beta \\rightarrow \\text{error vector}\n",
        "   $$\n",
        "\n",
        "3. Cost function in matrix form:\n",
        "   $$\n",
        "   (Y - X\\beta)^T(Y - X\\beta) = \\text{SSE}\n",
        "   $$\n",
        "\n",
        "4. Gradient of SSE:\n",
        "   $$\n",
        "   \\nabla_\\beta SSE = -2X^T(Y - X\\beta)\n",
        "   $$\n",
        "\n",
        "#### Gradient Descent Update Rule\n",
        "At each iteration:\n",
        "$$\n",
        "\\beta_{new} = \\beta_{old} - \\eta \\nabla_\\beta SSE\n",
        "$$\n",
        "Where:\n",
        "- $\\eta$ is the learning rate\n",
        "- $\\nabla_\\beta SSE$ is the gradient computed above\n",
        "\n",
        "#### Implementation Steps\n",
        "1. Initialize $\\beta$ with random values\n",
        "2. Compute predictions: $\\hat{y} = X\\beta$\n",
        "3. Calculate errors: $e = y - \\hat{y}$\n",
        "4. Compute gradient: $\\nabla_\\beta = -2X^Te$\n",
        "5. Update parameters: $\\beta = \\beta - \\eta \\nabla_\\beta$\n",
        "6. Repeat until convergence\n",
        "\n",
        "#### Convergence Criteria\n",
        "The algorithm stops when either:\n",
        "- $\\|\\nabla_\\beta\\| < \\epsilon$ (small threshold)\n",
        "- Maximum iterations reached\n",
        "- Change in SSE between iterations becomes negligible"
      ],
      "metadata": {
        "id": "lyh2shfg1S93"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbrIeiUv0fd6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d3e11db"
      },
      "source": [
        "$$\n",
        "Prediction=\\hat{Y}=X\\beta = \\begin{bmatrix}\n",
        "1 & x_{11} & \\cdots & x_{p1} \\\\\n",
        "1 & x_{12} & \\cdots & x_{p2} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{1n} & \\cdots & x_{pn}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\vdots \\\\\n",
        "\\beta_p\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{Y} = \\begin{bmatrix}\n",
        "1\\cdot\\beta_0 + x_{11}\\cdot\\beta_1 + \\cdots + x_{p1}\\cdot\\beta_p \\\\\n",
        "1\\cdot\\beta_0 + x_{12}\\cdot\\beta_1 + \\cdots + x_{p2}\\cdot\\beta_p \\\\\n",
        "\\vdots \\\\\n",
        "1\\cdot\\beta_0 + x_{1n}\\cdot\\beta_1 + \\cdots + x_{pn}\\cdot\\beta_p\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This results in a vector of predicted values, where each element corresponds to the predicted value for a single data point.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43863183"
      },
      "source": [
        "$$\n",
        "\\text{Error} = Y - X\\beta\n",
        "$$\n",
        "\n",
        "$$\n",
        "Error = \\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_n\n",
        "\\end{bmatrix} - \\left( \\begin{bmatrix}\n",
        "1 & x_{11} & \\cdots & x_{p1} \\\\\n",
        "1 & x_{12} & \\cdots & x_{p2} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{1n} & \\cdots & x_{pn}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\vdots \\\\\n",
        "\\beta_p\n",
        "\\end{bmatrix} \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "Error = \\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_n\n",
        "\\end{bmatrix} - \\begin{bmatrix}\n",
        "1\\cdot\\beta_0 + x_{11}\\cdot\\beta_1 + \\cdots + x_{p1}\\cdot\\beta_p \\\\\n",
        "1\\cdot\\beta_0 + x_{12}\\cdot\\beta_1 + \\cdots + x_{p2}\\cdot\\beta_p \\\\\n",
        "\\vdots \\\\\n",
        "1\\cdot\\beta_0 + x_{1n}\\cdot\\beta_1 + \\cdots + x_{pn}\\cdot\\beta_p\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "Error = \\begin{bmatrix}\n",
        "y_1 - (1\\cdot\\beta_0 + x_{11}\\cdot\\beta_1 + \\cdots + x_{p1}\\cdot\\beta_p) \\\\\n",
        "y_2 - (1\\cdot\\beta_0 + x_{12}\\cdot\\beta_1 + \\cdots + x_{p2}\\cdot\\beta_p) \\\\\n",
        "\\vdots \\\\\n",
        "y_n - (1\\cdot\\beta_0 + x_{1n}\\cdot\\beta_1 + \\cdots + x_{pn}\\cdot\\beta_p)\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "e_1 \\\\\n",
        "e_2 \\\\\n",
        "\\vdots \\\\\n",
        "e_n\n",
        "\\end{bmatrix} = e\n",
        "$$\n",
        "\n",
        "This results in a vector of errors, where each element is the difference between the actual value and the predicted value for a single data point."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vNfeUC-Y5wib"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1=np.random.randint(low=1,high=20,size=20000)\n",
        "x2=np.random.randint(low=1,high=20,size=20000)"
      ],
      "metadata": {
        "id": "qHfcjhgI7V5j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=3+2*x1-4*x2+np.random.random(20000)"
      ],
      "metadata": {
        "id": "QtObk57k7boG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you can see that we have generated data such that y is an approximate linear combination of x1 and x2, next we'll calculate optimal parameter values using gradient descent and compare them with results from sklearn and we'll see how good is the method."
      ],
      "metadata": {
        "id": "Y2AxYRTw7syL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=pd.DataFrame({'intercept':np.ones(x1.shape[0]),'x1':x1,'x2':x2})\n",
        "w=np.random.random(x.shape[1])"
      ],
      "metadata": {
        "id": "2lTqHkTO7hp3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpgF0BbP7nmT",
        "outputId": "b57d5cf9-f99c-42c0-cd21-ec361662c756"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 23.13792683, -20.18992061,   1.10753345, ...,  29.26322215,\n",
              "       -66.34913842, -60.2879041 ])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lets write functions for predictions, error, cost and gradient that we discussed above"
      ],
      "metadata": {
        "id": "hhinodvi78xX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Prediction"
      ],
      "metadata": {
        "id": "lX0t_H8g8Gu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def myprediction(features,weights):\n",
        "    predictions=np.dot(features,weights) ## function numpy.dot : is used for matrix multiplication\n",
        "    return(predictions)\n",
        "myprediction(x,w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vunY6Osg73kY",
        "outputId": "a5e24c82-94ce-4982-93ad-700419bcb1b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15.13518179, 12.67216884,  6.83351855, ..., 15.63173257,\n",
              "        7.68692004,  6.41593163])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. Error"
      ],
      "metadata": {
        "id": "zkqitZnp8b69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def myerror(target,features,weights):\n",
        "    error=target-myprediction(features,weights)\n",
        "    return(error)\n",
        "myerror(y,x,w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYQIl9gn8QTz",
        "outputId": "fa43003a-2178-44f7-83fb-03a66e197087"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  8.00274504, -32.86208945,  -5.72598511, ...,  13.63148958,\n",
              "       -74.03605846, -66.70383573])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Loss/Cost Function"
      ],
      "metadata": {
        "id": "CQCmr9fL8sAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mycost(target,features,weights):\n",
        "    error=myerror(target,features,weights)\n",
        "    cost=np.dot(error.T,error)\n",
        "    return(cost)\n",
        "mycost(y,x,w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLmZnAaF8hmP",
        "outputId": "4bae3f5d-649b-4008-83e7-0b64ceacce1c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(26785821.910174765)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4. Gradient of Loss"
      ],
      "metadata": {
        "id": "xZSjbgrj81Eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(target,features,weights):\n",
        "    error=myerror(target,features,weights)\n",
        "    gradient=-np.dot(features.T,error)/features.shape[0]\n",
        "    return(gradient)\n",
        "gradient(y,x,w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOHQOsHp8xkO",
        "outputId": "a1a0af71-8262-49c9-80cb-052e04a783c9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 27.22707814, 233.95898948, 402.00693839])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Note that gradient here is vector of 3 values because there are 3 parameters . Also since this is being evaluated on the entire data, we scaled it down with number of observations . Do recall that , the approximation which led to the ultimate results was that change in parameters is small. We dont have any direct control over gradient , we can always chose a small value for $\\eta$ to ensure that change in parameter remains small. Also if we end up chosing too small value for $\\eta$, we'll need to take larger number of steps to change in parameter in order to arrive at the optimal value of the parameters\n",
        "\n",
        "\n",
        "**Lets looks at the expected value for parameters from sklearn . Dont worry about the syntax here , we'll discuss that in detail, when we formally start with linear models in next module/Lecture .**"
      ],
      "metadata": {
        "id": "Bk4d3W3o9DHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "agj2fsrf89r4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=LinearRegression()\n",
        "lr.fit(x.iloc[:,1:],y)\n",
        "sk_estimates=([lr.intercept_]+list(lr.coef_))"
      ],
      "metadata": {
        "id": "1kAu9Cu39wm8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sk_estimates"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn9SonbZ9z3Y",
        "outputId": "03cf3ab0-376a-48b2-e98f-2db66f6cb6e2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[np.float64(3.4992084121435383),\n",
              " np.float64(1.9997501123748274),\n",
              " np.float64(-3.999658633868609)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you run the same , these might be different for you, as we generated the data randomly"
      ],
      "metadata": {
        "id": "Tpd3cSpA99XZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now lets write our version of this , using gradient descent"
      ],
      "metadata": {
        "id": "hfjKB_X5-Dt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_lr(target,features,learning_rate,num_steps,print_when):\n",
        "\n",
        "    # start with random values of parameters\n",
        "    weights=np.random.random(features.shape[1])\n",
        "\n",
        "    # change parameter multiple times in sequence\n",
        "    # using the cost function gradient which we discussed earlier\n",
        "    for i in range(num_steps):\n",
        "\n",
        "        weights =weights- learning_rate*gradient(target,features,weights)\n",
        "\n",
        "    # this simply prints the cost function value every (print_when)th iteration\n",
        "        if i%print_when==0:\n",
        "            print(mycost(target,features,weights),weights)\n",
        "\n",
        "    return(weights)"
      ],
      "metadata": {
        "id": "IXwwDigX92tS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_lr(y,x,.0001,500,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeGRE95x-J6h",
        "outputId": "4a99dd1f-37c4-43c4-d74a-b18030116f15"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34532015.67046891 [0.6051539  0.92387787 0.59690049]\n",
            "24710121.062121555 [0.5759674  0.66893269 0.17102219]\n",
            "18374086.185818285 [ 0.5529527   0.48223191 -0.18121298]\n",
            "14233481.138368469 [ 0.53482384  0.34910705 -0.47469547]\n",
            "11478878.032792915 [ 0.52056273  0.25795879 -0.7212273 ]\n",
            "9602363.894890811 [ 0.5093634   0.19961717 -0.93016508]\n",
            "8285041.1582575375 [ 0.50058776  0.16683516 -1.10892898]\n",
            "7326514.18275353 [ 0.49373065  0.15388772 -1.26340575]\n",
            "6600691.52223081 [ 0.48839216  0.15625436 -1.39826779]\n",
            "6028082.258197277 [ 0.48425568  0.17036788 -1.51722574]\n",
            "5558430.444725748 [ 0.48107056  0.19341543 -1.62322848]\n",
            "5159829.283368368 [ 0.4786384   0.22318105 -1.71862151]\n",
            "4811896.700032572 [ 0.47680209  0.25792099 -1.80527226]\n",
            "4501496.579963767 [ 0.47543729  0.29626503 -1.88466941]\n",
            "4220055.631063833 [ 0.47444555  0.33713835 -1.95800146]\n",
            "3961880.4193658573 [ 0.47374894  0.37969973 -2.02621897]\n",
            "3723101.357282534 [ 0.47328577  0.42329257 -2.09008384]\n",
            "3501009.7171368822 [ 0.47300722  0.46740616 -2.15020832]\n",
            "3293641.046882538 [ 0.47287461  0.51164501 -2.20708592]\n",
            "3099513.0850422904 [ 0.47285736  0.55570459 -2.26111589]\n",
            "2917460.5691148737 [ 0.47293125  0.59935215 -2.31262259]\n",
            "2746530.8284228016 [ 0.47307711  0.64241146 -2.36187083]\n",
            "2585917.526100789 [ 0.47327977  0.68475087 -2.40907808]\n",
            "2434918.360056383 [ 0.47352723  0.72627371 -2.45442403]\n",
            "2292907.8260773765 [ 0.47380998  0.76691084 -2.49805831]\n",
            "2159319.464142358 [ 0.47412051  0.80661467 -2.54010647]\n",
            "2033634.0887005518 [ 0.47445287  0.84535446 -2.58067482]\n",
            "1915371.8073373144 [ 0.47480237  0.88311259 -2.61985421]\n",
            "1804086.4494711123 [ 0.47516528  0.91988164 -2.65772308]\n",
            "1699361.53907477 [ 0.47553867  0.95566208 -2.69434982]\n",
            "1600807.2666613907 [ 0.4759202   0.99046041 -2.72979469]\n",
            "1508058.117240102 [ 0.47630805  1.02428775 -2.76411132]\n",
            "1420770.9373283258 [ 0.47670076  1.05715867 -2.79734793]\n",
            "1338623.303422354 [ 0.4770972   1.08909032 -2.82954828]\n",
            "1261312.1041371496 [ 0.47749646  1.12010172 -2.86075242]\n",
            "1188552.279536272 [ 0.47789782  1.1502132  -2.89099732]\n",
            "1120075.680882675 [ 0.47830074  1.17944598 -2.92031734]\n",
            "1055630.0264753625 [ 0.47870476  1.20782183 -2.94874462]\n",
            "994977.9371061632 [ 0.47910955  1.2353628  -2.97630942]\n",
            "937896.0396745584 [ 0.47951484  1.26209102 -3.00304033]\n",
            "884174.1307018712 [ 0.47992041  1.28802854 -3.0289645 ]\n",
            "833614.393557162 [ 0.4803261   1.31319724 -3.05410782]\n",
            "786030.6645647783 [ 0.48073179  1.33761868 -3.07849502]\n",
            "741247.7440704443 [ 0.48113738  1.36131409 -3.1021498 ]\n",
            "699100.7491639608 [ 0.48154278  1.3843043  -3.12509493]\n",
            "659434.5051955384 [ 0.48194795  1.40660967 -3.14735231]\n",
            "622102.9735446153 [ 0.48235285  1.42825012 -3.16894304]\n",
            "586968.7133457151 [ 0.48275743  1.44924509 -3.18988748]\n",
            "553902.3750712158 [ 0.48316169  1.46961351 -3.21020528]\n",
            "522782.2240322974 [ 0.4835656   1.48937385 -3.22991544]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.48392881,  1.50665309, -3.24715025])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sk_estimates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74ReA_Fp-PTa",
        "outputId": "d01579c8-6aa6-47ef-d77a-9e0df9505ef3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[np.float64(3.4992084121435383), np.float64(1.9997501123748274), np.float64(-3.999658633868609)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- you can see that if we take too few steps , we did not reach to the optimal value.\n",
        "- Lets increase the learning rate $\\eta$"
      ],
      "metadata": {
        "id": "3UIwtd7M-pwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_lr(y,x,.01,500,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAGXZd1l-XzL",
        "outputId": "e40a1a74-c53c-4ec0-d77b-41d892f0bae2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23389875.033597946 [ 0.11976344 -1.21799464 -3.35836761]\n",
            "4095597047.7425246 [ -1.54191766 -18.88094798 -24.87771922]\n",
            "890778349496.3228 [ -26.64405545 -307.10496343 -314.58821204]\n",
            "193742740198188.44 [ -397.41668255 -4558.5088672  -4586.4137325 ]\n",
            "4.213870999945231e+16 [ -5866.06845981 -67257.45439613 -67586.49321834]\n",
            "9.165096348118702e+18 [ -86517.32933034 -991930.34453477 -996700.45262407]\n",
            "1.993392561648011e+21 [ -1275948.35017788 -14628841.8906572  -14699108.09982173]\n",
            "4.3355942522625986e+23 [-1.88174664e+07 -2.15743592e+08 -2.16779783e+08]\n",
            "9.429842310995687e+25 [-2.77516659e+08 -3.18174820e+09 -3.19702968e+09]\n",
            "2.050974349452536e+28 [-4.09276641e+09 -4.69238575e+10 -4.71492259e+10]\n",
            "4.460833642156739e+30 [-6.03593922e+10 -6.92024717e+11 -6.95348411e+11]\n",
            "9.702235812119734e+32 [-8.90169597e+11 -1.02058576e+13 -1.02548749e+13]\n",
            "2.110219463563442e+35 [-1.31280631e+13 -1.50514176e+14 -1.51237074e+14]\n",
            "4.58969073792193e+37 [-1.93610344e+14 -2.21975634e+15 -2.23041750e+15]\n",
            "9.982497760775227e+39 [-2.85533097e+15 -3.27365722e+16 -3.28938012e+16]\n",
            "2.1711759513670616e+42 [-4.21099140e+16 -4.82793153e+17 -4.85111939e+17]\n",
            "4.722270041790197e+44 [-6.21029533e+17 -7.12014769e+18 -7.15434472e+18]\n",
            "1.027085544750452e+47 [-9.15883327e+18 -1.05006674e+20 -1.05511005e+20]\n",
            "2.2338932481621234e+49 [-1.35072846e+20 -1.54861977e+21 -1.55605757e+21]\n",
            "4.858679074678969e+51 [-1.99203035e+21 -2.28387694e+22 -2.29484607e+22]\n",
            "1.0567542728438334e+54 [-2.93781099e+22 -3.36822115e+23 -3.38439823e+23]\n",
            "2.2984222172515095e+56 [-4.33263147e+23 -4.96739274e+24 -4.99125040e+24]\n",
            "4.9990284633900296e+58 [-6.38968795e+24 -7.32582260e+25 -7.36100745e+25]\n",
            "1.0872800214952306e+61 [-9.42339831e+25 -1.08039931e+27 -1.08558831e+27]\n",
            "2.3648151911921747e+63 [-1.38974605e+27 -1.59335371e+28 -1.60100636e+28]\n",
            "5.14343203032706e+65 [-2.04957279e+28 -2.34984976e+29 -2.36113574e+29]\n",
            "1.118687546880044e+68 [-3.02267355e+29 -3.46551669e+30 -3.48216106e+30]\n",
            "2.433126014236271e+70 [-4.45778528e+30 -5.11088247e+31 -5.13542929e+31]\n",
            "5.2920068858048355e+72 [-6.57426256e+31 -7.53743870e+32 -7.57363992e+32]\n",
            "1.1510023202886223e+75 [-9.69560567e+32 -1.11160807e+34 -1.11694696e+34]\n",
            "2.5034100859986423e+77 [-1.42989071e+34 -1.63937985e+35 -1.64725355e+35]\n",
            "5.4448735230249045e+79 [-2.10877742e+35 -2.41772828e+36 -2.42934028e+36]\n",
            "1.184250548783374e+82 [-3.10998749e+36 -3.56562273e+37 -3.58274790e+37]\n",
            "2.575724406385479e+84 [-4.58655432e+37 -5.25851708e+38 -5.28377298e+38]\n",
            "5.602155915794598e+86 [-6.76416885e+38 -7.75516761e+39 -7.79241455e+39]\n",
            "1.2184591964523962e+89 [-9.97567610e+39 -1.14371835e+41 -1.14921146e+41]\n",
            "2.6501276218207943e+91 [-1.47119500e+41 -1.68673551e+42 -1.69483665e+42]\n",
            "5.763981619069331e+93 [-2.16969225e+42 -2.48756757e+43 -2.49951499e+43]\n",
            "1.253656006277251e+96 [-3.19982360e+43 -3.66862047e+44 -3.68624032e+44]\n",
            "2.7266800728084158e+98 [-4.71904302e+44 -5.41041632e+45 -5.43640177e+45]\n",
            "5.930481872398344e+100 [-6.95956084e+45 -7.97918591e+46 -8.01750878e+46]\n",
            "1.2898695226323346e+103 [-1.02638367e+47 -1.17675617e+48 -1.18240796e+48]\n",
            "2.8054438428675853e+105 [-1.51369241e+48 -1.73545910e+49 -1.74379426e+49]\n",
            "6.101791706359329e+107 [-2.23236669e+49 -2.55942426e+50 -2.57171680e+50]\n",
            "1.3271291144341344e+110 [-3.29225475e+50 -3.77459344e+51 -3.79272227e+51]\n",
            "2.886482808882378e+112 [-4.85535884e+51 -5.56670338e+52 -5.59343944e+52]\n",
            "6.278050052067454e+114 [-7.16059699e+52 -8.20967528e+53 -8.24910515e+53]\n",
            "1.365464998959243e+117 [-1.05603213e+54 -1.21074833e+55 -1.21656338e+55]\n",
            "2.969862692905366e+119 [-1.55741743e+55 -1.78559014e+56 -1.79416607e+56]\n",
            "6.4593998538473685e+121 [-2.29685156e+56 -2.63335662e+57 -2.64600425e+57]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.58814163e+57, 2.96732275e+58, 2.98157437e+58])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that because of high learning rate , change is parameter is huge and we end up missing the optimal point , cost function values , as well as parameter values ended up exploding. Now lets run with low learning rate and higher number of steps"
      ],
      "metadata": {
        "id": "176Uce0Z-_st"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_lr(y,x,.0004,100000,1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raFoob7V-3AW",
        "outputId": "d12be09d-4ba2-4762-c127-19fde02a0868"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25574783.82825318 [0.70964133 0.05579706 0.53542758]\n",
            "21283.562193351743 [ 0.75550049  2.11972474 -3.88089352]\n",
            "19349.647105502303 [ 0.8942895   2.11367155 -3.88691677]\n",
            "17606.43595106173 [ 1.02605792  2.10790889 -3.89261976]\n",
            "16035.12309290925 [ 1.15116092  2.10243774 -3.89803426]\n",
            "14618.757615952734 [ 1.26993564  2.09724334 -3.90317488]\n",
            "13342.060137213162 [ 1.38270221  2.0923117  -3.90805546]\n",
            "12191.25797580888 [ 1.48976454  2.08762952 -3.91268915]\n",
            "11153.936576850552 [ 1.59141118  2.08318419 -3.91708846]\n",
            "10218.905586446532 [ 1.68791608  2.07896372 -3.92126523]\n",
            "9376.078133071082 [ 1.77953933  2.07495674 -3.92523071]\n",
            "8616.362013013495 [ 1.86652787  2.07115245 -3.92899561]\n",
            "7931.561606045468 [ 1.94911614  2.0675406  -3.93257006]\n",
            "7314.289463198152 [ 2.02752673  2.06411146 -3.9359637 ]\n",
            "6757.886612881024 [ 2.10197096  2.06085577 -3.93918568]\n",
            "6256.350725625576 [ 2.17264947  2.05776478 -3.94224467]\n",
            "5804.271362513569 [ 2.23975274  2.05483014 -3.94514892]\n",
            "5396.7716087666995 [ 2.30346163  2.05204394 -3.94790627]\n",
            "5029.455462856695 [ 2.36394784  2.04939869 -3.95052413]\n",
            "4698.360413582253 [ 2.42137439  2.04688724 -3.95300958]\n",
            "4399.91469352686 [ 2.47589604  2.04450283 -3.95536929]\n",
            "4130.898747759802 [ 2.52765975  2.04223904 -3.95760965]\n",
            "3888.41050211322 [ 2.57680502  2.04008976 -3.95973667]\n",
            "3669.834056359802 [ 2.6234643  2.0380492 -3.9617561]\n",
            "3472.8114645603346 [ 2.66776335  2.03611186 -3.96367338]\n",
            "3295.217298155205 [ 2.70982155  2.03427252 -3.96549368]\n",
            "3135.135717392935 [ 2.74975226  2.03252623 -3.96722189]\n",
            "2990.839803748546 [ 2.7876631   2.03086826 -3.96886269]\n",
            "2860.772930374823 [ 2.82365624  2.02929417 -3.97042048]\n",
            "2743.531969616187 [ 2.85782868  2.0277997  -3.97189948]\n",
            "2637.8521564317903 [ 2.89027253  2.02638083 -3.97330366]\n",
            "2542.593444438853 [ 2.92107523  2.02503373 -3.97463681]\n",
            "2456.7282073880806 [ 2.95031979  2.02375477 -3.97590253]\n",
            "2379.3301533987596 [ 2.97808502  2.0225405  -3.97710422]\n",
            "2309.564332362361 [ 3.00444577  2.02138766 -3.97824512]\n",
            "2246.678128717521 [ 3.02947307  2.02029314 -3.97932831]\n",
            "2189.993142428998 [ 3.05323438  2.01925398 -3.98035671]\n",
            "2138.897870583989 [ 3.07579374  2.01826739 -3.98133309]\n",
            "2092.841110657735 [ 3.09721194  2.0173307  -3.98226008]\n",
            "2051.326014283652 [ 3.11754672  2.0164414  -3.98314017]\n",
            "2013.9047273822625 [ 3.13685287  2.01559708 -3.98397575]\n",
            "1980.1735588276274 [ 3.15518243  2.01479547 -3.98476906]\n",
            "1949.768625532306 [ 3.1725848   2.01403441 -3.98552224]\n",
            "1922.3619269711526 [ 3.18910688  2.01331184 -3.98623732]\n",
            "1897.6578067967498 [ 3.2047932   2.01262583 -3.98691623]\n",
            "1875.389763375575 [ 3.21968604  2.01197452 -3.9875608 ]\n",
            "1855.317574837503 [ 3.23382553  2.01135615 -3.98817276]\n",
            "1837.2247076247363 [ 3.24724978  2.01076907 -3.98875377]\n",
            "1820.915980583909 [ 3.25999497  2.01021168 -3.98930539]\n",
            "1806.215459402612 [ 3.27209546  2.00968249 -3.9898291 ]\n",
            "1792.964558675597 [ 3.28358384  2.00918006 -3.99032632]\n",
            "1781.020331126674 [ 3.2944911   2.00870306 -3.99079839]\n",
            "1770.2539255305876 [ 3.30484661  2.00825018 -3.99124658]\n",
            "1760.5491966994798 [ 3.3146783  2.0078202 -3.9916721]\n",
            "1751.8014525387339 [ 3.32401266  2.00741198 -3.9920761 ]\n",
            "1743.9163246558683 [ 3.33287484  2.00702441 -3.99245965]\n",
            "1736.8087503388006 [ 3.34128873  2.00665644 -3.99282381]\n",
            "1730.4020549214215 [ 3.34927702  2.00630709 -3.99316955]\n",
            "1724.6271246371825 [ 3.35686122  2.00597541 -3.99349779]\n",
            "1719.4216610377518 [ 3.36406177  2.00566051 -3.99380944]\n",
            "1714.7295089335198 [ 3.37089809  2.00536153 -3.99410532]\n",
            "1710.5000506059969 [ 3.3773886   2.00507768 -3.99438623]\n",
            "1706.687659756998 [ 3.38355079  2.00480819 -3.99465293]\n",
            "1703.2512093040093 [ 3.38940127  2.00455233 -3.99490614]\n",
            "1700.1536277118848 [ 3.39495581  2.00430941 -3.99514654]\n",
            "1697.3614990747521 [ 3.40022937  2.00407878 -3.99537478]\n",
            "1694.8447026339024 [ 3.40523617  2.00385982 -3.99559148]\n",
            "1692.5760878428723 [ 3.40998971  2.00365193 -3.99579721]\n",
            "1690.5311814743802 [ 3.41450279  2.00345456 -3.99599254]\n",
            "1688.6879236095228 [ 3.41878757  2.00326717 -3.99617799]\n",
            "1687.0264296610928 [ 3.42285562  2.00308926 -3.99635406]\n",
            "1685.5287758638076 [ 3.42671788  2.00292035 -3.99652122]\n",
            "1684.1788059174037 [ 3.43038478  2.00275999 -3.99667992]\n",
            "1682.961956696642 [ 3.43386619  2.00260774 -3.9968306 ]\n",
            "1681.865101148108 [ 3.43717149  2.00246319 -3.99697365]\n",
            "1680.8764066789606 [ 3.44030959  2.00232595 -3.99710947]\n",
            "1679.9852075100032 [ 3.44328896  2.00219565 -3.99723842]\n",
            "1679.181889616017 [ 3.44611761  2.00207194 -3.99736084]\n",
            "1678.4577870121605 [ 3.44880318  2.00195449 -3.99747708]\n",
            "1677.8050882675693 [ 3.4513529   2.00184299 -3.99758743]\n",
            "1677.216752237671 [ 3.45377365  2.00173712 -3.9976922 ]\n",
            "1676.6864321061473 [ 3.45607194  2.00163661 -3.99779167]\n",
            "1676.2084069171315 [ 3.45825398  2.00154118 -3.99788611]\n",
            "1675.7775198590105 [ 3.46032564  2.00145058 -3.99797577]\n",
            "1675.3891226340902 [ 3.4622925   2.00136456 -3.9980609 ]\n",
            "1675.0390253139603 [ 3.46415987  2.0012829  -3.99814172]\n",
            "1674.7234511396427 [ 3.46593278  2.00120536 -3.99821845]\n",
            "1674.4389957788876 [ 3.46761601  2.00113175 -3.9982913 ]\n",
            "1674.182590601129 [ 3.4692141   2.00106186 -3.99836047]\n",
            "1673.9514695738894 [ 3.47073134  2.00099551 -3.99842614]\n",
            "1673.743139423544 [ 3.47217184  2.00093251 -3.99848848]\n",
            "1673.5553527385255 [ 3.47353947  2.0008727  -3.99854767]\n",
            "1673.3860837248387 [ 3.47483792  2.00081591 -3.99860387]\n",
            "1673.2335063523178 [ 3.47607069  2.000762   -3.99865722]\n",
            "1673.0959746558885 [ 3.4772411   2.00071081 -3.99870788]\n",
            "1672.9720049793232 [ 3.4783523   2.00066222 -3.99875597]\n",
            "1672.8602599699414 [ 3.4794073   2.00061608 -3.99880163]\n",
            "1672.7595341515976 [ 3.48040893  2.00057227 -3.99884499]\n",
            "1672.6687409203184 [ 3.48135989  2.00053069 -3.99888614]\n",
            "1672.5869008223026 [ 3.48226275  2.0004912  -3.99892522]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.4831191 ,  2.00045375, -3.99896228])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "171e9d7a"
      },
      "source": [
        "We can see here that we ended up getting pretty good estimates for $\\beta$s, as good as from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sk_estimates"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L7LySbb_zex",
        "outputId": "a9181905-ed77-4b07-c6a0-0488c37d7961"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[np.float64(3.4992084121435383),\n",
              " np.float64(1.9997501123748274),\n",
              " np.float64(-3.999658633868609)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "there are modifications to gradient descent which can achieve the same thing in much less number of iterations. We'll discuss that in detail when we start with our course in Deep learning. For now ,we'll conclude this module"
      ],
      "metadata": {
        "id": "VHLvWHrW__Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKKP3WMD_6Qx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}